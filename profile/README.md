<h1 align="center">Learning and Vision Efficiency Research</h1>

Introduction for efficient deep learning research at [Learning and Vision Lab](http://lv-nus.org/), National University of Singapore.
<div align="center">
  <img src="https://github.com/horseee/Diffusion_DeepCache/blob/master/static/images/example_compress.gif" width="48%" ></img>
  <img src="https://github.com/czg1225/SlimSAM/blob/master/images/paper/everything.PNG" width="48%" ></img>
  <br>
  <div align="center">
  <img src="https://github.com/horseee/DeepCache/raw/master/assets/svd.gif" width="96%" ></img>
  <br>
  <em>
      (1.7x training-free acceleration of Stable Video Diffusion-XT with DeepCache) 
  </em>
</div>
</div>


## Papers
- [[CVPR'23] DepGraph: Towards Any Structural Pruning](#-depgraph-towards-any-structural-pruning)  [![Star](https://img.shields.io/github/stars/vainf/torch-pruning.svg?style=social&label=Star)](https://github.com/vainf/torch-pruning)
- [[NeurIPS'23] LLM-Pruner: On the Structural Pruning of Large Language Models](#-llm-pruner-on-the-structural-pruning-of-large-language-models) [![Star](https://img.shields.io/github/stars/horseee/LLM-Pruner.svg?style=social&label=Star)](https://github.com/horseee/LLM-Pruner)
- [[NeurIPS'23] Structural Pruning for Diffusion Models](#-structural-pruning-for-diffusion-models) [![Star](https://img.shields.io/github/stars/VainF/Diff-Pruning.svg?style=social&label=Star)](https://github.com/VainF/Diff-Pruning)
- [[CVPR'24] DeepCache: Accelerating Diffusion Models for Free](#-deepcache-accelerating-diffusion-models-for-free) [![Star](https://img.shields.io/github/stars/horseee/DeepCache.svg?style=social&label=Star)](https://github.com/horseee/DeepCache)
- [[ECCV'24] Isomorphic Pruning for Vision Models](#-isomorphic-pruning-for-vision-models) [![Star](https://img.shields.io/github/stars/VainF/Isomorphic-Pruning.svg?style=social&label=Star)](https://github.com/VainF/Isomorphic-Pruning)
- [SlimSAM: 0.1% Data Makes Segment Anything Slim](#-slimsam-01-data-makes-segment-anything-slim) [![Star](https://img.shields.io/github/stars/czg1225/SlimSAM.svg?style=social&label=Star)](https://github.com/czg1225/SlimSAM)
- [AsyncDiff: Parallelizing Diffusion Models by Asynchronous Denoising](#-asyncdiff-parallelizing-diffusion-models-by-asynchronous-denoising) [![Star](https://img.shields.io/github/stars/czg1225/AsyncDiff.svg?style=social&label=Star)](https://github.com/czg1225/AsyncDiff)
- [Learning-to-Cache: Accelerating Diffusion Transformer via Layer Caching]() [![Star](https://img.shields.io/github/stars/horseee/learning-to-cache.svg?style=social&label=Star)](https://github.com/horseee/learning-to-cache)

## 


### ðŸŒŸ DepGraph: Towards Any Structural Pruning  
> [![Star](https://img.shields.io/github/stars/vainf/torch-pruning.svg?style=social&label=Star)](https://github.com/vainf/torch-pruning) [![Publish](https://img.shields.io/badge/conference-CVPR'23-red)]() <a href="https://pepy.tech/project/Torch-Pruning"><img src="https://static.pepy.tech/badge/Torch-Pruning?color=2196f3" alt="Downloads"></a>  
> **TL;DR**: A *general* and *fully automatic* method, Dependency Graph (DepGraph), to explicitly model the dependency between layers and comprehensively group coupled parameters for pruning.  
> *[Gongfan Fang](https://fangggf.github.io/), [Xinyin Ma](https://horseee.github.io/), [Mingli Song](https://person.zju.edu.cn/en/msong), [Michael Bi Mi](https://dblp.org/pid/317/0937.html), [Xinchao Wang](https://sites.google.com/site/sitexinchaowang/)*     
> [[Paper]](https://openaccess.thecvf.com/content/CVPR2023/html/Fang_DepGraph_Towards_Any_Structural_Pruning_CVPR_2023_paper.html) [[GitHub]](https://github.com/vainf/torch-pruning) 
> <details> <summary>Abstract:</summary> Diffusion models have recently gained unprecedented attention in the field of image synthesis due to their remarkable generative capabilities. Notwithstanding their prowess, these models often incur substantial computational costs, primarily attributed to the sequential denoising process and cumbersome model size. Traditional methods for compressing diffusion models typically involve extensive retraining, presenting cost and feasibility challenges. In this paper, we introduce DeepCache, a novel training-free paradigm that accelerates diffusion models from the perspective of model architecture. DeepCache capitalizes on the inherent temporal redundancy observed in the sequential denoising steps of diffusion models, which caches and retrieves features across adjacent denoising stages, thereby curtailing redundant computations. Utilizing the property of the U-Net, we reuse the high-level features while updating the low-level features in a very cheap way. This innovative strategy, in turn, enables a speedup factor of 2.3X for Stable Diffusion v1.5 with only a 0.05 decline in CLIP Score, and 4.1X for LDM-4-G with a slight decrease of 0.22 in FID on ImageNet. Our experiments also demonstrate DeepCache's superiority over existing pruning and distillation methods that necessitate retraining and its compatibility with current sampling techniques. Furthermore, we find that under the same throughput, DeepCache effectively achieves comparable or even marginally improved results with DDIM or PLMS. </details>

### ðŸŒŸ LLM-Pruner: On the Structural Pruning of Large Language Models
> [![Star](https://img.shields.io/github/stars/horseee/LLM-Pruner.svg?style=social&label=Star)](https://github.com/horseee/LLM-Pruner) [![Publish](https://img.shields.io/badge/Conference-NeurIPS'23-red)]()  
> **TL;DR**: Compress your LLMs to any size! A task-agnostic compression framework with only 3 minutes for pruning and 3 hours for post-training.  
> *[Xinyin Ma](https://horseee.github.io/), [Gongfan Fang](https://fangggf.github.io/), [Xinchao Wang](https://sites.google.com/site/sitexinchaowang/)*  
> [[Paper]](https://arxiv.org/abs/2305.11627) [[GitHub]](https://github.com/horseee/LLM-Pruner)  
> <details> <summary>Abstract:</summary> Large language models (LLMs) have shown remarkable capabilities in language understanding and generation. However, such impressive capability typically comes with a substantial model size, which presents significant challenges in both the deployment, inference, and training stages. With LLM being a general-purpose task solver, we explore its compression in a task-agnostic manner, which aims to preserve the multi-task solving and language generation ability of the original LLM. One challenge to achieving this is the enormous size of the training corpus of LLM, which makes both data transfer and model post-training over-burdensome. Thus, we tackle the compression of LLMs within the bound of two constraints: being task-agnostic and minimizing the reliance on the original training dataset. Our method, named LLM-Pruner, adopts structural pruning that selectively removes non-critical coupled structures based on gradient information, maximally preserving the majority of the LLM's functionality. To this end, the performance of pruned models can be efficiently recovered through tuning techniques, LoRA, in merely 3 hours, requiring only 50K data. We validate the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate that the compressed models still exhibit satisfactory capabilities in zero-shot classification and generation. </details>

### ðŸŒŸ Structural Pruning for Diffusion Models
> [![Star](https://img.shields.io/github/stars/Vainf/Diff-Pruning.svg?style=social&label=Star)](https://github.com/Vainf/Diff-Pruning) [![Publish](https://img.shields.io/badge/Conference-NeurIPS'23-red)]()    
> **TL;DR**: An efficient compression method tailored for learning lightweight diffusion models from pre-existing ones with two benefits: Efficiency and Consistency.    
> *[Gongfan Fang](https://fangggf.github.io/), [Xinyin Ma](https://horseee.github.io/), [Xinchao Wang](https://sites.google.com/site/sitexinchaowang/)*  
> [[Paper]](https://arxiv.org/abs/2305.10924) [[GitHub]](https://github.com/vainf/Diff-Pruning)  
>  <details> <summary>Abstract:</summary> Generative modeling has recently undergone remarkable advancements, primarily propelled by the transformative implications of Diffusion Probabilistic Models (DPMs). The impressive capability of these models, however, often entails significant computational overhead during both training and inference. To tackle this challenge, we present Diff-Pruning, an efficient compression method tailored for learning lightweight diffusion models from pre-existing ones, without the need for extensive re-training. The essence of Diff-Pruning is encapsulated in a Taylor expansion over pruned timesteps, a process that disregards non-contributory diffusion steps and ensembles informative gradients to identify important weights. Our empirical assessment, undertaken across several datasets highlights two primary benefits of our proposed method: 1) Efficiency: it enables approximately a 50% reduction in FLOPs at a mere 10% to 20% of the original training expenditure; 2) Consistency: the pruned diffusion models inherently preserve generative behavior congruent with their pre-trained models. </details>

### ðŸŒŸ DeepCache: Accelerating Diffusion Models for Free
> [![Star](https://img.shields.io/github/stars/horseee/DeepCache.svg?style=social&label=Star)](https://github.com/horseee/DeepCache)  [![Publish](https://img.shields.io/badge/Conference-CVPR'24-red)]() <a href="https://pepy.tech/project/DeepCache"><img src="https://static.pepy.tech/badge/DeepCache?color=2196f3" alt="Downloads"></a>    
> **TL;DR**: A training-free paradigm that accelerates diffusion models. 2.3Ã— speedup for Stable Diffusion v1.5 and a 4.1Ã— speedup for LDM-4-G, based upon DDIM/PLMS.   
> *[Xinyin Ma](https://horseee.github.io/), [Gongfan Fang](https://fangggf.github.io/), [Xinchao Wang](https://sites.google.com/site/sitexinchaowang/)*  
> [[Paper]](https://arxiv.org/abs/2312.00858) [[GitHub]](https://github.com/horseee/DeepCache)  [[Project Page]](https://horseee.github.io/Diffusion_DeepCache/) 
> <details> <summary>Abstract:</summary> Diffusion models have recently gained unprecedented attention in the field of image synthesis due to their remarkable generative capabilities. Notwithstanding their prowess, these models often incur substantial computational costs, primarily attributed to the sequential denoising process and cumbersome model size. Traditional methods for compressing diffusion models typically involve extensive retraining, presenting cost and feasibility challenges. In this paper, we introduce DeepCache, a novel training-free paradigm that accelerates diffusion models from the perspective of model architecture. DeepCache capitalizes on the inherent temporal redundancy observed in the sequential denoising steps of diffusion models, which caches and retrieves features across adjacent denoising stages, thereby curtailing redundant computations. Utilizing the property of the U-Net, we reuse the high-level features while updating the low-level features in a very cheap way. This innovative strategy, in turn, enables a speedup factor of 2.3X for Stable Diffusion v1.5 with only a 0.05 decline in CLIP Score, and 4.1X for LDM-4-G with a slight decrease of 0.22 in FID on ImageNet. Our experiments also demonstrate DeepCache's superiority over existing pruning and distillation methods that necessitate retraining and its compatibility with current sampling techniques. Furthermore, we find that under the same throughput, DeepCache effectively achieves comparable or even marginally improved results with DDIM or PLMS. </details>

### ðŸŒŸ Isomorphic Pruning for Vision Models
> [![Star](https://img.shields.io/github/stars/VainF/Isomorphic-Pruning.svg?style=social&label=Star)](https://github.com/VainF/Isomorphic-Pruning) [![Publish](https://img.shields.io/badge/Conference-ECCV'24-red)]()      
> **TL;DR**: We present Isomorphic Pruning, a method that performs isolated ranking and comparison on different types of sub-structures for more reliable pruning.  
> *[Gongfan Fang](https://fangggf.github.io/), [Xinyin Ma](https://horseee.github.io/), [Michael Bi Mi](https://dblp.org/pid/317/0937.html), [Xinchao Wang](https://sites.google.com/site/sitexinchaowang/)*   
> [[Paper]](https://arxiv.org/abs/2407.04616) [[GitHub]](https://github.com/VainF/Isomorphic-Pruning)    
> <details> <summary>Abstract:</summary> Structured pruning reduces the computational overhead of deep neural networks by removing redundant sub-structures. However, assessing the relative importance of different sub-structures remains a significant challenge, particularly in advanced vision models featuring novel mechanisms and architectures like self-attention, depth-wise convolutions, or residual connections. These heterogeneous substructures usually exhibit diverged parameter scales, weight distributions, and computational topology, introducing considerable difficulty to importance comparison. To overcome this, we present Isomorphic Pruning, a simple approach that demonstrates effectiveness across a range of network architectures such as Vision Transformers and CNNs, and delivers competitive performance across different model sizes. Isomorphic Pruning originates from an observation that, when evaluated under a pre-defined importance criterion, heterogeneous sub-structures demonstrate significant divergence in their importance distribution, as opposed to isomorphic structures that present similar importance patterns. This inspires us to perform isolated ranking and comparison on different types of sub-structures for more reliable pruning. Our empirical results on ImageNet-1K demonstrate that Isomorphic Pruning surpasses several pruning baselines dedicatedly designed for Transformers or CNNs. For instance, we improve the accuracy of DeiT-Tiny from 74.52% to 77.50% by pruning an off-the-shelf DeiT-Base model. And for ConvNext-Tiny, we enhanced performance from 82.06% to 82.18%, while reducing the number of parameters and memory usage. </details>


### ðŸŒŸ SlimSAM: 0.1% Data Makes Segment Anything Slim
> [![Star](https://img.shields.io/github/stars/czg1225/SlimSAM.svg?style=social&label=Star)](https://github.com/czg1225/SlimSAM)    
> **TL;DR**:  SlimSAM is a data-efficient SAM compression method, which offers exceptional performance with significantly low training costs (only 0.1% data).  
> *[Zigeng Chen](https://github.com/czg1225), [Gongfan Fang](https://fangggf.github.io/), [Xinyin Ma](https://horseee.github.io/), [Xinchao Wang](https://sites.google.com/site/sitexinchaowang/)*  
> [[Paper]](https://arxiv.org/abs/2312.05284) [[GitHub]](https://github.com/czg1225/SlimSAM)  
> <details> <summary>Abstract:</summary> The formidable model size and demanding computational requirements of Segment Anything Model (SAM) have rendered it cumbersome for deployment on resource-constrained devices. Existing approaches for SAM compression typically involve training a new network from scratch, posing a challenging trade-off between compression costs and model performance. To address this issue, this paper introduces SlimSAM, a novel SAM compression method that achieves superior performance with remarkably low training costs. This is achieved by the efficient reuse of pre-trained SAMs through a unified pruning-distillation framework. To enhance knowledge inheritance from the original SAM, we employ an innovative alternate slimming strategy that partitions the compression process into a progressive procedure. Diverging from prior pruning techniques, we meticulously prune and distill decoupled model structures in an alternating fashion. Furthermore, a novel label-free pruning criterion is also proposed to align the pruning objective with the optimization target, thereby boosting the post-distillation after pruning. SlimSAM yields significant performance improvements while demanding over 10 times less training costs than any other existing methods. Even when compared to the original SAM-H, SlimSAM achieves approaching performance while reducing parameter counts to merely 0.9% (5.7M), MACs to 0.8% (21G), and requiring only 0.1% (10k) of the SAM training data. </details>

### ðŸŒŸ AsyncDiff: Parallelizing Diffusion Models by Asynchronous Denoising
> [![Star](https://img.shields.io/github/stars/czg1225/AsyncDiff.svg?style=social&label=Star)](https://github.com/czg1225/AsyncDiff)  
> **TL;DR**: A universal and plug-and-play diffusion acceleration scheme that enables model parallelism across multiple devices.   
> *[Zigeng Chen](https://github.com/czg1225), [Xinyin Ma](https://horseee.github.io/), [Gongfan Fang](https://fangggf.github.io/), [Zhenxiong Tan](https://github.com/Yuanshi9815), [Xinchao Wang](https://sites.google.com/site/sitexinchaowang/)*  
> [[Paper]](https://arxiv.org/abs/2406.06911) [[GitHub]](https://github.com/czg1225/AsyncDiff)  [[Project Page]](https://czg1225.github.io/asyncdiff_page/) 
> <details> <summary>Abstract:</summary> Diffusion models have garnered significant interest from the community for their great generative ability across various applications. However, their typical multi-step sequential-denoising nature gives rise to high cumulative latency, thereby precluding the possibilities of parallel computation. To address this, we introduce AsyncDiff, a universal and plug-and-play acceleration scheme that enables model parallelism across multiple devices. Our approach divides the cumbersome noise prediction model into multiple components, assigning each to a different device. To break the dependency chain between these components, it transforms the conventional sequential denoising into an asynchronous process by exploiting the high similarity between hidden states in consecutive diffusion steps. Consequently, each component is facilitated to compute in parallel on separate devices. The proposed strategy significantly reduces inference latency while minimally impacting the generative quality. Specifically, for the Stable Diffusion v2.1, AsyncDiff achieves a 2.7x speedup with negligible degradation and a 4.0x speedup with only a slight reduction of 0.38 in CLIP Score, on four NVIDIA A5000 GPUs. Our experiments also demonstrate that AsyncDiff can be readily applied to video diffusion models with encouraging performances. </details>

### ðŸŒŸ Learning-to-Cache: Accelerating Diffusion Transformer via Layer Caching
> [![Star](https://img.shields.io/github/stars/horseee/learning-to-cache.svg?style=social&label=Star)](https://github.com/horseee/learning-to-cache)  
> **TL;DR**: Learning-to-Cache learns to conduct caching dynamically for diffusion transformers.  
> *[Xinyin Ma](https://horseee.github.io/), [Gongfan Fang](https://fangggf.github.io/), [Michael Bi Mi](https://dblp.org/pid/317/0937.html), [Xinchao Wang](https://sites.google.com/site/sitexinchaowang/)*  
> [[Paper]](https://arxiv.org/abs/2406.01733) [[GitHub]](https://github.com/horseee/learning-to-cache)
> <details> <summary>Abstract:</summary> Diffusion Transformers have recently demonstrated unprecedented generative capabilities for various tasks. The encouraging results, however, come with the cost of slow inference, since each denoising step requires inference on a transformer model with a large scale of parameters. In this study, we make an interesting and somehow surprising observation: the computation of a large proportion of layers in the diffusion transformer, through introducing a caching mechanism, can be readily removed even without updating the model parameters. In the case of U-ViT-H/2, for example, we may remove up to 93.68% of the computation in the cache steps (46.84% for all steps), with less than 0.01 drop in FID. To achieve this, we introduce a novel scheme, named Learning-to-Cache (L2C), that learns to conduct caching in a dynamic manner for diffusion transformers. Specifically, by leveraging the identical structure of layers in transformers and the sequential nature of diffusion, we explore redundant computations between timesteps by treating each layer as the fundamental unit for caching. To address the challenge of the exponential search space in deep models for identifying layers to cache and remove, we propose a novel differentiable optimization objective. An input-invariant yet timestep-variant router is then optimized, which can finally produce a static computation graph. Experimental results show that L2C largely outperforms samplers such as DDIM and DPM-Solver, alongside prior cache-based methods at the same inference speed. </details>




